{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–æ–æ–±—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è NER-–∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö:\n",
    "\n",
    "1. –û–±—É—á–∏—Ç–µ NER-–º–æ–¥–µ–ª—å\n",
    "\n",
    "- –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö [Collection5](https://github.com/natasha/corus?tab=readme-ov-file#load_ne5) - **1 –±–∞–ª–ª**\n",
    "- –†–∞–∑–±–µ–π—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/test —á–∞—Å—Ç–∏\n",
    "- –î–æ–æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å [rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2) –Ω–∞ train-—á–∞—Å—Ç–∏ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è NER-–∑–∞–¥–∞—á–∏, —Å–¥–µ–ª–∞–π—Ç–µ –∑–∞–º–µ—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞ NER-–º–µ—Ç—Ä–∏–∫ –¥–æ –∏ –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è - **2 –±–∞–ª–ª–∞**\n",
    "\n",
    "2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏:\n",
    "- –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –¥–æ–æ–±—É—á–∏—Ç–µ –Ω–∞ train-—á–∞—Å—Ç–∏ –≤ MLM —Ä–µ–∂–∏–º–µ, –∞ –ø–æ—Ç–æ–º –¥–æ–æ–±—É—á–∏—Ç–µ –Ω–∞ NER-–∑–∞–¥–∞—á—É - **2 –±–∞–ª–ª–∞**\n",
    "- –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–º–µ—Ç–∫—É* –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ**, –Ω–∞ –≤–∞—à –≤–∑–≥–ª—è–¥, –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –±–æ–ª—å—à–æ–π –∏ —É–º–Ω–æ–π –º–æ–¥–µ–ª—å—é –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ NER***, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤ –µ–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è rubert-tiny2 –≤–º–µ—Å—Ç–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö - **2 –±–∞–ª–ª–∞**\n",
    "\n",
    "3. –§–∏–Ω–∞–ª—å–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ - **1 –±–∞–ª–ª**\n",
    "\n",
    "*–ø—Ä–æ–≥–æ–Ω–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —á–µ—Ä–µ–∑ NER-–º–æ–¥–µ–ª—å, –ø–æ–ª—É—á–∏—Ç–µ –µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Ö –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–∑–º–µ—Ç–∫–∏\n",
    "\n",
    "**–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–∂–µ –∑–Ω–∞–∫–æ–º—ã–π –≤–∞–º –¥–∞—Ç–∞—Å–µ—Ç lenta-ru, –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ –≤–∑—è—Ç—å –æ—Ç 10_000 —Ç–µ–∫—Å—Ç–æ–≤\n",
    "\n",
    "***–ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –º–æ–¥–µ–ª—å –º–æ–¥–µ–ª—å DeepPavlov ner_collection3_bert. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∑–∞–ø—É—Å–∫—É –µ—Å—Ç—å –≤ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏](https://docs.deeppavlov.ai/en/master/features/models/NER.html)\n",
    "\n",
    "**–û–±—â–µ–µ**\n",
    "\n",
    "- –ü—Ä–∏–Ω–∏–º–∞–µ–º—ã–µ —Ä–µ—à–µ–Ω–∏—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω—ã (–ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞/–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä/–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä/–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Ç.–ø.) - **1 –±–∞–ª–ª**\n",
    "- –û–±–µ—Å–ø–µ—á–µ–Ω–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è: –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã random_state, –Ω–æ—É—Ç–±—É–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞ –±–µ–∑ –æ—à–∏–±–æ–∫ - **1 –±–∞–ª–ª**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 0 - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "from corus import load_ne5, load_lenta\n",
    "from datasets import Dataset, ClassLabel, Sequence, concatenate_datasets\n",
    "from natasha import Segmenter, NewsEmbedding, NewsNERTagger, Doc\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForMaskedLM,\n",
    ")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(device)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§–∏–∫—Å–∏—Ä—É–µ–º seed'—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 1 - –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_ne5(\"../data/hw_4/Collection5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ne5Markup(id='001', text='–†–æ—Å—Å–∏—è —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –°–®–ê –Ω–∞ –ì—Ä—É–∑–∏—é\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\n–ú–û–°–ö–í–ê, 4 –∞–≤–≥ - –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏. –†–æ—Å—Å–∏—è —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç, —á—Ç–æ –°–®–ê –≤–æ–∑–¥–µ–π—Å—Ç–≤—É—é—Ç –Ω–∞ –¢–±–∏–ª–∏—Å–∏ –≤ —Å–≤—è–∑–∏ —Å –æ–±–æ—Å—Ç—Ä–µ–Ω–∏–µ–º —Å–∏—Ç—É–∞—Ü–∏–∏ –≤ –∑–æ–Ω–µ –≥—Ä—É–∑–∏–Ω–æ-–æ—Å–µ—Ç–∏–Ω—Å–∫–æ–≥–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞. –û–± —ç—Ç–æ–º —Å—Ç–∞—Ç—Å-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å - –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å –º–∏–Ω–∏—Å—Ç—Ä–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –¥–µ–ª –†–æ—Å—Å–∏–∏ –ì—Ä–∏–≥–æ—Ä–∏–π –ö–∞—Ä–∞—Å–∏–Ω –∑–∞—è–≤–∏–ª –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–µ —Å –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª–µ–º –≥–æ—Å—Å–µ–∫—Ä–µ—Ç–∞—Ä—è –°–®–ê –î—ç–Ω–∏—ç–ª–æ–º –§—Ä–∏–¥–æ–º.\\r\\n\\r\\n\"–° —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –≤—ã—Ä–∞–∂–µ–Ω–∞ –≥–ª—É–±–æ–∫–∞—è –æ–∑–∞–±–æ—á–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–≤—è–∑–∏ —Å –Ω–æ–≤—ã–º –≤–∏—Ç–∫–æ–º –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ–∫—Ä—É–≥ –Æ–∂–Ω–æ–π –û—Å–µ—Ç–∏–∏, –ø—Ä–æ—Ç–∏–≤–æ–∑–∞–∫–æ–Ω–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –≥—Ä—É–∑–∏–Ω—Å–∫–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ –Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏—é —Å–≤–æ–∏—Ö –≤–æ–æ—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏–ª –≤ —Ä–µ–≥–∏–æ–Ω–µ, –±–µ—Å–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ–º —Ñ–æ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π\", - –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏.\\r\\n\\r\\n\"–†–æ—Å—Å–∏—è —É–∂–µ –ø—Ä–∏–∑–≤–∞–ª–∞ –¢–±–∏–ª–∏—Å–∏ –∫ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏ –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–∞–∫–∂–µ –Ω–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –í–∞—à–∏–Ω–≥—Ç–æ–Ω–∞\", - —Å–æ–æ–±—â–∏–ª –ú–ò–î –†–æ—Å—Å–∏–∏. ', spans=[Ne5Span(index='T1', type='GEOPOLIT', start=0, stop=6, text='–†–æ—Å—Å–∏—è'), Ne5Span(index='T2', type='GEOPOLIT', start=50, stop=53, text='–°–®–ê'), Ne5Span(index='T3', type='GEOPOLIT', start=57, stop=63, text='–ì—Ä—É–∑–∏—é'), Ne5Span(index='T4', type='LOC', start=87, stop=93, text='–ú–û–°–ö–í–ê'), Ne5Span(index='T5', type='MEDIA', start=103, stop=114, text='–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏'), Ne5Span(index='T6', type='GEOPOLIT', start=116, stop=122, text='–†–æ—Å—Å–∏—è'), Ne5Span(index='T7', type='GEOPOLIT', start=141, stop=144, text='–°–®–ê'), Ne5Span(index='T8', type='GEOPOLIT', start=161, stop=168, text='–¢–±–∏–ª–∏—Å–∏'), Ne5Span(index='T9', type='GEOPOLIT', start=301, stop=307, text='–†–æ—Å—Å–∏–∏'), Ne5Span(index='T10', type='PER', start=308, stop=324, text='–ì—Ä–∏–≥–æ—Ä–∏–π –ö–∞—Ä–∞—Å–∏–Ω'), Ne5Span(index='T11', type='GEOPOLIT', start=383, stop=386, text='–°–®–ê'), Ne5Span(index='T12', type='PER', start=387, stop=402, text='–î—ç–Ω–∏—ç–ª–æ–º –§—Ä–∏–¥–æ–º'), Ne5Span(index='T13', type='GEOPOLIT', start=505, stop=517, text='–Æ–∂–Ω–æ–π –û—Å–µ—Ç–∏–∏'), Ne5Span(index='T14', type='GEOPOLIT', start=703, stop=709, text='–†–æ—Å—Å–∏—è'), Ne5Span(index='T15', type='GEOPOLIT', start=723, stop=730, text='–¢–±–∏–ª–∏—Å–∏'), Ne5Span(index='T16', type='GEOPOLIT', start=815, stop=825, text='–í–∞—à–∏–Ω–≥—Ç–æ–Ω–∞'), Ne5Span(index='T17', type='ORG', start=838, stop=841, text='–ú–ò–î'), Ne5Span(index='T18', type='GEOPOLIT', start=842, stop=848, text='–†–æ—Å—Å–∏–∏')])\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "* –ë—É–¥–µ–º —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ - –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –∫ –º–µ—Ç–∫–µ.\n",
    "* –î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞—à —Ç–µ–∫—Å—Ç –¥–æ –≤–∏–¥–∞ list —Ç–æ–∫–µ–Ω–æ–≤, list –º–µ—Ç–æ–∫.\n",
    "* –ú–µ—Ç–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º –ø–æ —Å—Ö–µ–º–µ BIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_markup_to_ner(item):\n",
    "    text = item.text\n",
    "    tokens = []\n",
    "    offsets = []\n",
    "    for match in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(match.group())\n",
    "        offsets.append((match.start(), match.end()))\n",
    "\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for span in item.spans:\n",
    "        token_indices = [\n",
    "            i for i, (tstart, tend) in enumerate(offsets) if not (tend <= span.start or tstart >= span.stop)\n",
    "        ]\n",
    "        if token_indices:\n",
    "            labels[token_indices[0]] = \"B-\" + span.type\n",
    "            for idx in token_indices[1:]:\n",
    "                labels[idx] = \"I-\" + span.type\n",
    "\n",
    "    return {\"id\": item.id, \"tokens\": tokens, \"ner_tags\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [convert_markup_to_ner(item) for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = Dataset.from_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for example in ner_dataset:\n",
    "    unique_labels.update(example[\"ner_tags\"])\n",
    "\n",
    "unique_labels = sorted(list(unique_labels))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-GEOPOLIT': 0,\n",
       " 'B-LOC': 1,\n",
       " 'B-MEDIA': 2,\n",
       " 'B-ORG': 3,\n",
       " 'B-PER': 4,\n",
       " 'I-GEOPOLIT': 5,\n",
       " 'I-LOC': 6,\n",
       " 'I-MEDIA': 7,\n",
       " 'I-ORG': 8,\n",
       " 'I-PER': 9,\n",
       " 'O': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(example):\n",
    "    example[\"ner_tags\"] = [label_to_id[label] for label in example[\"ner_tags\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b813daaa787b4e7894eeaacea8b30cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_dataset = ner_dataset.map(convert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4a9a25a84e472cbccda2156850ee1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = ner_dataset.features.copy()\n",
    "features[\"ner_tags\"] = Sequence(ClassLabel(names=unique_labels))\n",
    "ner_dataset = ner_dataset.cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '002', 'tokens': ['–ö–æ–º–∏—Å—Å–∞—Ä', '–°–ï', '–∫—Ä–∏—Ç–∏–∫—É–µ—Ç', '–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å–Ω—É—é', '–ø–æ–ª–∏—Ç–∏–∫—É', '–≤', '–æ—Ç–Ω–æ—à–µ–Ω–∏–∏', '–±–µ–∂–µ–Ω—Ü–µ–≤', '–≤', '–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏—Ö', '—Å—Ç—Ä–∞–Ω–∞—Ö', '05/08/2008', '10:32', '–ú–û–°–ö–í–ê,', '5', '–∞–≤–≥—É—Å—Ç–∞', '/–ù–æ–≤–æ—Å—Ç–∏-–ì—Ä—É–∑–∏—è/.', '–ü—Ä–æ–≤–æ–¥–∏–º–∞—è', '–≤', '–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏—Ö', '—Å—Ç—Ä–∞–Ω–∞—Ö', '–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å–Ω–∞—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–≤', '–æ—Ç–Ω–æ—à–µ–Ω–∏–∏', '–±–µ–∂–µ–Ω—Ü–µ–≤', '–Ω–∞—Ä—É—à–∞–µ—Ç', '—Ä—è–¥', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤,', '–≤', '—á–∞—Å—Ç–Ω–æ—Å—Ç–∏,', '–ø—Ä–∞–≤–æ', '–Ω–∞', '–≤–æ—Å—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ', '—Å–µ–º–µ–π,', '–∑–∞—è–≤–ª—è–µ—Ç', '–ö–æ–º–∏—Å—Å–∞—Ä', '–°–æ–≤–µ—Ç–∞', '–ï–≤—Ä–æ–ø—ã', '–ø–æ', '–ø—Ä–∞–≤–∞–º', '—á–µ–ª–æ–≤–µ–∫–∞', '–¢–æ–º–∞—Å', '–•–∞–º–º–∞—Ä–±–µ—Ä–≥', '(Thomas', 'Hammarberg)', '–≤', '—Ä–∞–∑–º–µ—â–µ–Ω–Ω–æ–º', '–Ω–∞', '–µ–≥–æ', '—Å–∞–π—Ç–µ', '–µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–º', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.', '\"–û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å–Ω–∞—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–≤', '–æ—Ç–Ω–æ—à–µ–Ω–∏–∏', '–±–µ–∂–µ–Ω—Ü–µ–≤', '–≤', '–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏—Ö', '—Å—Ç—Ä–∞–Ω–∞—Ö', '—É–º–µ–Ω—å—à–∞–µ—Ç', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏', '–≤–æ—Å—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è', '—Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö', '—Å–µ–º–µ–π\",', '-', '–ø–æ–ª–∞–≥–∞–µ—Ç', '–æ–Ω.', '–ü–æ', '—Å–æ–æ–±—â–µ–Ω–∏—é', '–†–ò–ê', '–ù–æ–≤–æ—Å—Ç–∏,', '–•–∞–º–º–∞—Ä–±–µ—Ä–≥', '–∫–æ–Ω—Å—Ç–∞—Ç–∏—Ä—É–µ—Ç,', '—á—Ç–æ', '–≤', '–ø–æ—Å–ª–µ–¥–Ω–µ–µ', '–≤—Ä–µ–º—è', '\"–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞', '–ø–æ–ø—ã—Ç–∞–ª–∏—Å—å', '–æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å', '–ø—Ä–∏–µ–∑–¥', '–±–ª–∏–∑–∫–∏—Ö', '—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤', '–∫', '—Ç–µ–º', '–±–µ–∂–µ–Ω—Ü–∞–º,', '–∫–æ—Ç–æ—Ä—ã–µ', '—É–∂–µ', '–ø—Ä–æ–∂–∏–≤–∞—é—Ç', '–≤', '—Å—Ç—Ä–∞–Ω–µ\".', '–ö–æ–º–∏—Å—Å–∞—Ä', '–Ω–µ', '–Ω–∞–∑—ã–≤–∞–µ—Ç', '–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö', '—Å—Ç—Ä–∞–Ω,', '–æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ', '–æ—Ç–º–µ—á–∞—è,', '—á—Ç–æ', '–≤', '—Ä—è–¥–µ', '—Å–ª—É—á–∞–µ–≤', '–ø–æ–¥–æ–±–Ω–∞—è', '–ª–∏–Ω–∏—è', '–ø—Ä–∏–≤–µ–ª–∞', '\"–∫', '–Ω–µ–æ–ø—Ä–∞–≤–¥–∞–Ω–Ω—ã–º', '—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º', '—Å—Ç—Ä–∞–¥–∞–Ω–∏—è–º,', '–∫–æ–≥–¥–∞', '—á–ª–µ–Ω—ã', '—Å–µ–º—å–∏,', '–∑–∞–≤–∏—Å—è—â–∏–µ', '–¥—Ä—É–≥', '–æ—Ç', '–¥—Ä—É–≥–∞,', '–æ–∫–∞–∑–∞–ª–∏—Å—å', '—Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º–∏\".', '\"–¢–∞–∫–∞—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç', '–ø—Ä–∞–≤—É', '–Ω–∞', '–≤–æ—Å—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ', '—Å–µ–º–µ–π,', '–∫–∞–∫', '—ç—Ç–æ', '–ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–æ', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–º–∏', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏\",', '-', '–∑–∞–º–µ—á–∞–µ—Ç', '–æ–Ω.', '–ö–æ–º–∏—Å—Å–∞—Ä', '–°–æ–≤–µ—Ç–∞', '–ï–≤—Ä–æ–ø—ã', '–ø—Ä–∏–∑—ã–≤–∞–µ—Ç', '—Å—Ç—Ä–∞–Ω—ã', '—É—á–∏—Ç—ã–≤–∞—Ç—å', '–≤', '–ø–æ–ª–∏—Ç–∏–∫–µ,', '–ø—Ä–æ–≤–æ–¥–∏–º–æ–π', '–≤', '–æ—Ç–Ω–æ—à–µ–Ω–∏–∏', '–±–µ–∂–µ–Ω—Ü–µ–≤,', '–ø–æ–ª–æ–∂–µ–Ω–∏—è', '–æ', '—Å–µ–º—å–µ,', '–ø—Ä–∏–Ω—è—Ç—ã–µ', '–≤', '—Ä–∞–º–∫–∞—Ö', '–û–û–ù', '–∏', '–ï–°.'], 'ner_tags': [10, 3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 2, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 8, 10, 10, 10, 4, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 2, 7, 4, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 10, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(ner_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "\n",
    "–û—Å—Ç–∞–≤–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ—Ç–º–µ—Ç–∏–º -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a098efe434d4f549971e1d3dd06d028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = ner_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 2 - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞—á—É NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(unique_labels)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "–ö–∞–∂–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—Å—è –∑–∞ 10 —ç–ø–æ—Ö, –Ω–æ transformers —Å–æ—Ö—Ä–∞–Ω–∏—Ç –ª—É—á—à–∏–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞ –Ω–∞—Å :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emperor\\Documents\\GitHub\\AI-2024-2_Sem\\DL NLP\\.venv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/dbert\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è sequence labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [unique_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [unique_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–∏–º –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emperor\\AppData\\Local\\Temp\\ipykernel_13768\\1377821198.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.364283561706543, 'eval_model_preparation_time': 0.0021, 'eval_precision': 0.0151832055155318, 'eval_recall': 0.077500988533017, 'eval_f1': 0.02539189014121, 'eval_accuracy': 0.10832422481735028, 'eval_runtime': 0.4656, 'eval_samples_per_second': 429.544, 'eval_steps_per_second': 27.92}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation before fine-tuning:\")\n",
    "pre_training_results = trainer.evaluate()\n",
    "print(pre_training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883037</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.566144</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.277066</td>\n",
       "      <td>0.247924</td>\n",
       "      <td>0.261686</td>\n",
       "      <td>0.835069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434527</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.381899</td>\n",
       "      <td>0.440490</td>\n",
       "      <td>0.409108</td>\n",
       "      <td>0.875511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.361122</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.538299</td>\n",
       "      <td>0.605773</td>\n",
       "      <td>0.570047</td>\n",
       "      <td>0.908244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.314260</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.603787</td>\n",
       "      <td>0.680902</td>\n",
       "      <td>0.640030</td>\n",
       "      <td>0.921935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279122</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.713325</td>\n",
       "      <td>0.681526</td>\n",
       "      <td>0.930794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.675520</td>\n",
       "      <td>0.744168</td>\n",
       "      <td>0.708184</td>\n",
       "      <td>0.937353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243194</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.684814</td>\n",
       "      <td>0.756030</td>\n",
       "      <td>0.718662</td>\n",
       "      <td>0.940114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236398</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.763938</td>\n",
       "      <td>0.724274</td>\n",
       "      <td>0.940862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.233604</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.690519</td>\n",
       "      <td>0.763147</td>\n",
       "      <td>0.725019</td>\n",
       "      <td>0.941725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emperor\\Documents\\GitHub\\AI-2024-2_Sem\\DL NLP\\.venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4705190124511719, metrics={'train_runtime': 15.3424, 'train_samples_per_second': 520.779, 'train_steps_per_second': 32.589, 'total_flos': 14148015982080.0, 'train_loss': 0.4705190124511719, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation after fine-tuning:\n",
      "{'eval_loss': 0.2336038202047348, 'eval_model_preparation_time': 0.0021, 'eval_precision': 0.6905187835420393, 'eval_recall': 0.7631474891261368, 'eval_f1': 0.7250187828700226, 'eval_accuracy': 0.9417246735316114, 'eval_runtime': 0.5124, 'eval_samples_per_second': 390.31, 'eval_steps_per_second': 25.37, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation after fine-tuning:\")\n",
    "post_training_results = trainer.evaluate()\n",
    "print(post_training_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É\n",
    "\n",
    "* –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ, —á–µ–º –¥–æ, —á—Ç–æ –æ—á–µ–≤–∏–¥–Ω–æ.\n",
    "* –°—É–¥—è –ø–æ val_loss –º–æ–¥–µ–ª—å –Ω–µ–º–Ω–æ–≥–æ –Ω–µ–¥–æ–æ–±—É—á–∏–ª–∞—Å—å.\n",
    "* –ü–æ–ª—É—á–∏–ª–∏ –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π `accuracy` = 0.94, —á—Ç–æ –ø–æ–Ω—è—Ç–Ω–æ –ø–æ –º–µ—Ç–∫–µ 'O' - –ø—É—Å—Ç–æ–π, –∏—Ö –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ.\n",
    "* –ü–æ–ª—É—á–∏–ª–∏ –≤–ø–æ–ª–Ω–µ –Ω–µ–ø–ª–æ—Ö–æ–π `f1` = 0.725 –Ω–∞ —Ç–µ—Å—Ç–µ, –±—É–¥–µ–º –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —ç—Ç–æ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 3 - –ü—Ä–µ–¥–æ–±—É—á–∏–º –Ω–∞ MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ\n",
    "\n",
    "–°–∫–ª–µ–∏–º —Ç–µ–∫—Å—Ç—ã, —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏—Ö –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74147c12acdd4f89a351fedf0d178852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def join_tokens(example):\n",
    "    example[\"text\"] = \" \".join(example[\"tokens\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "mlm_train_dataset = train_dataset.map(join_tokens)\n",
    "mlm_test_dataset = test_dataset.map(join_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270174a0cbde4726a5fe0f7a83973fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_for_mlm(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "\n",
    "mlm_train_dataset = mlm_train_dataset.map(tokenize_for_mlm, batched=True)\n",
    "mlm_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "mlm_test_dataset = mlm_test_dataset.map(tokenize_for_mlm, batched=True)\n",
    "mlm_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–∑–¥–∞–¥–∏–º –º–æ–¥–µ–ª—å –∏ –æ–±—É—á–∏–º –µ—ë\n",
    "\n",
    "–í–æ–∑—å–º—ë–º –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö, –Ω–æ —á—É—Ç—å –ø–æ–≤—ã—Å–∏–º LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_training_args = TrainingArguments(\n",
    "    output_dir=\"../models/mlm/\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "mlm_trainer = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=mlm_training_args,\n",
    "    train_dataset=mlm_train_dataset,\n",
    "    eval_dataset=mlm_test_dataset,\n",
    "    data_collator=mlm_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.945902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.821543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.891276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=3.11281616924799, metrics={'train_runtime': 25.5505, 'train_samples_per_second': 105.555, 'train_steps_per_second': 6.693, 'total_flos': 20582750048256.0, 'train_loss': 3.11281616924799, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –≤—ã–≤–æ–¥\n",
    "\n",
    "–£—Å–ø–µ—à–Ω–æ –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å MLM, –ø—Ä–æ–≤–µ—Ä–∏–º –Ω–∞ NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–∞–µ–º NER –º–æ–¥–µ–ª—å —Å –º–æ–º–µ–Ω—Ç–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ MLM\n",
    "\n",
    "–û—Å—Ç–∞–≤–∏–º –≤—Å–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –Ω–æ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../models/mlm/checkpoint-171/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"../models/mlm/checkpoint-171/\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_training_args = TrainingArguments(\n",
    "    output_dir=\"../models/mlm/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "ner_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=ner_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation before NER fine-tuning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.375126600265503, 'eval_model_preparation_time': 0.0016, 'eval_precision': 0.027367955149011507, 'eval_recall': 0.14669829972321075, 'eval_f1': 0.04612993472179049, 'eval_accuracy': 0.12535235574987055, 'eval_runtime': 0.4322, 'eval_samples_per_second': 462.793, 'eval_steps_per_second': 30.082}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation before NER fine-tuning:\")\n",
    "print(ner_trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.898690</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573792</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.249110</td>\n",
       "      <td>0.270154</td>\n",
       "      <td>0.833688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.438180</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.469182</td>\n",
       "      <td>0.496639</td>\n",
       "      <td>0.482520</td>\n",
       "      <td>0.878905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.360574</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.622776</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.908186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313215</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.624777</td>\n",
       "      <td>0.691973</td>\n",
       "      <td>0.656660</td>\n",
       "      <td>0.924064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276746</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.662281</td>\n",
       "      <td>0.716489</td>\n",
       "      <td>0.688319</td>\n",
       "      <td>0.932233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.256119</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.668948</td>\n",
       "      <td>0.734282</td>\n",
       "      <td>0.700094</td>\n",
       "      <td>0.936490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243475</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.673506</td>\n",
       "      <td>0.739818</td>\n",
       "      <td>0.705106</td>\n",
       "      <td>0.938100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.237623</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.747331</td>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.938963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.234957</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.677996</td>\n",
       "      <td>0.749308</td>\n",
       "      <td>0.711871</td>\n",
       "      <td>0.939539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.478190673828125, metrics={'train_runtime': 15.0348, 'train_samples_per_second': 531.434, 'train_steps_per_second': 33.256, 'total_flos': 14148015982080.0, 'train_loss': 0.478190673828125, 'epoch': 10.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "* –ú–æ–¥–µ–ª—å –Ω–µ–º–Ω–æ–≥–æ –Ω–µ–¥–æ—É—á–∏–ª–∞—Å—å, —Ä–æ–≤–Ω–æ –∫–∞–∫ –∏ –≤ –ø–µ—Ä–≤–æ–º —Å–ª—É—á–∞–µ.\n",
    "* –ü–æ–ª—É—á–∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —á—É—Ç—å —Ö—É–∂–µ –ø–æ `f1` = 0.712, –Ω–æ, –≤–ø–æ–ª–Ω–µ –≤–µ—Ä–æ—è—Ç–Ω–æ, –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã–ª–∏ –±—ã +- —Å—Ö–æ–∂–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 4 - –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ NER\n",
    "\n",
    "* –ò—Å–ø–æ–ª—å–∑—É–µ–º 10_000 —Å—ç–º–ø–ª–æ–≤ lenta\n",
    "* –í –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑—å–º—ë–º NewsNERTagger –∏–∑ natasha, –æ–Ω –∏–º–µ–µ—Ç —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –±–µ–Ω—á–º–∞—Ä–∫–∞–º –ø—Ä–∏ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lenta = load_lenta(\"../data/raw/lenta-ru-news.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\"title\", \"topic\", \"text\"]\n",
    "data_dict = {c: [] for c in target_columns}\n",
    "\n",
    "for item in data_lenta:\n",
    "    for column in target_columns:\n",
    "        data_dict[column].append(eval(f\"item.{column}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict)\n",
    "df = df.sample(10_000, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–º–µ—á–µ–Ω–Ω—ã–π natasha-—Å–µ—Ç–∞–ø "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "embedding = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markup_natasha(example):\n",
    "    text = example[\"text\"]\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "\n",
    "    tokens = []\n",
    "    offsets = []\n",
    "    for match in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(match.group())\n",
    "        offsets.append((match.start(), match.end()))\n",
    "\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for span in doc.spans:\n",
    "        token_indices = [i for i, (s, e) in enumerate(offsets) if not (e <= span.start or s >= span.stop)]\n",
    "        if token_indices:\n",
    "            labels[token_indices[0]] = \"B-\" + span.type\n",
    "            for idx in token_indices[1:]:\n",
    "                labels[idx] = \"I-\" + span.type\n",
    "\n",
    "    return {\"tokens\": tokens, \"ner_tags\": labels, \"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9852419fd5a245cb9e20986054a55f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markup_dataset = dataset.map(create_markup_natasha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∏–≤–µ–¥—ë–º –ª–µ–π–±–ª—ã –∫ –≤–∏–¥—É –Ω–∞—à–µ–≥–æ train –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce671a0455ac4abc87c25cec28d7347d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_list = [\n",
    "    \"B-GEOPOLIT\",\n",
    "    \"B-LOC\",\n",
    "    \"B-MEDIA\",\n",
    "    \"B-ORG\",\n",
    "    \"B-PER\",\n",
    "    \"I-GEOPOLIT\",\n",
    "    \"I-LOC\",\n",
    "    \"I-MEDIA\",\n",
    "    \"I-ORG\",\n",
    "    \"I-PER\",\n",
    "    \"O\",\n",
    "]\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "\n",
    "def convert_tags(example):\n",
    "    example[\"ner_tags\"] = [label_map[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "markup_dataset = markup_dataset.map(convert_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f17efcb930e44898cfa1c7f8453b3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = markup_dataset.features.copy()\n",
    "features[\"ner_tags\"] = Sequence(feature=ClassLabel(names=label_list))\n",
    "markup_dataset = markup_dataset.cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch[\"tokens\"], truncation=True, padding=\"max_length\", is_split_into_words=True, max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(batch[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594c0cea91bd4ed596c9eb8eb96270e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markup_dataset = markup_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "markup_dataset = markup_dataset.remove_columns([\"title\", \"topic\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_dataset = concatenate_datasets([train_dataset, markup_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–∏–º –º–æ–¥–µ–ª—å\n",
    "\n",
    "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã –ø—Ä–µ–¥—ã–¥—É—â–∏–º –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for example in combined_train_dataset:\n",
    "    unique_labels.update(example[\"ner_tags\"])\n",
    "unique_labels = sorted(list(unique_labels))\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_aug(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        pred_labels = []\n",
    "        lab_labels = []\n",
    "        for p_val, l_val in zip(pred, lab):\n",
    "            if l_val != -100:\n",
    "                pred_labels.append(label_list[p_val])\n",
    "                lab_labels.append(label_list[l_val])\n",
    "        true_predictions.append(pred_labels)\n",
    "        true_labels.append(lab_labels)\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_training_args = TrainingArguments(\n",
    "    output_dir=\"../models/augmented/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "aug_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=aug_training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation before Aug fine-tuning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.503415822982788, 'eval_model_preparation_time': 0.0, 'eval_precision': 0.019561815336463225, 'eval_recall': 0.09578544061302682, 'eval_f1': 0.032488628979857055, 'eval_accuracy': 0.06508063593732129, 'eval_runtime': 0.5807, 'eval_samples_per_second': 344.396, 'eval_steps_per_second': 22.386}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation before Aug fine-tuning:\")\n",
    "print(aug_trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6750/6750 02:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>0.263587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>0.682759</td>\n",
       "      <td>0.667791</td>\n",
       "      <td>0.932060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.216964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678821</td>\n",
       "      <td>0.706130</td>\n",
       "      <td>0.692207</td>\n",
       "      <td>0.937722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.182285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694239</td>\n",
       "      <td>0.720307</td>\n",
       "      <td>0.707033</td>\n",
       "      <td>0.941153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.167431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701548</td>\n",
       "      <td>0.729502</td>\n",
       "      <td>0.715252</td>\n",
       "      <td>0.944584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.158705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.721275</td>\n",
       "      <td>0.745594</td>\n",
       "      <td>0.733233</td>\n",
       "      <td>0.947615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.147120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.746026</td>\n",
       "      <td>0.773180</td>\n",
       "      <td>0.759360</td>\n",
       "      <td>0.951618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.145638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752501</td>\n",
       "      <td>0.778161</td>\n",
       "      <td>0.765116</td>\n",
       "      <td>0.952877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.141626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758442</td>\n",
       "      <td>0.783142</td>\n",
       "      <td>0.770594</td>\n",
       "      <td>0.954135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.139693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762963</td>\n",
       "      <td>0.789272</td>\n",
       "      <td>0.775895</td>\n",
       "      <td>0.955107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.139085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762593</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.775518</td>\n",
       "      <td>0.955221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6750, training_loss=0.06993983395894368, metrics={'train_runtime': 158.8719, 'train_samples_per_second': 679.73, 'train_steps_per_second': 42.487, 'total_flos': 191219555182080.0, 'train_loss': 0.06993983395894368, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "* –ú–æ–¥–µ–ª—å –æ–ø—è—Ç—å –Ω–µ–º–Ω–æ–≥–æ –Ω–µ–¥–æ—É—á–∏–ª–∞—Å—å\n",
    "* –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ, –æ—Å–æ–±–µ–Ω–Ω–æ –≤—ã—Ä–æ—Å `precision`\n",
    "* `f1` = 0.776 - —ç—Ç–æ —è–≤–Ω—ã–π –ª–∏–¥–µ—Ä –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã\n",
    "\n",
    "* –°–º–æ–≥–ª–∏ –æ–±—É—á–∏—Ç—å –≤–µ—Å—å–º–∞ –Ω–µ–ø–ª–æ—Ö—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –¥–æ—É—á–∏—Ç—å, —Ç–æ –±—ã–ª –±—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª `f1` ~= 0.8)\n",
    "* –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∞ —Å–µ–±—è –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –ª–∏—á–Ω—ã–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—ã—Ç–æ–º\n",
    "* –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ MLM –≤ –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ –æ–∫–∞–∑–∞–ª–æ—Å—å –Ω–µ –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
