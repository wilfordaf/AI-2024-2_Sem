{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "В этом задании вам предстоит продолжить работу с задачей машинного перевода из [занятия 7](https://github.com/pacifikus/itmo_dl_nlp_course/blob/main/Lecture%207/itmo_dl_nlp_course_06_seq2seq.ipynb)\n",
    "\n",
    "Попробуйте улучшить качество модели, проверив следующие гипотезы:\n",
    "\n",
    "- измените размер словаря / предобработку во время токенизации - **1 балл**\n",
    "- продолжите эксперименты с различными RNN юнитами в encoder и decoder части: замена GRU/LSTM, изменение количества слоев, использование bidirectional RNN - **1 балл**\n",
    "- улучшите процесс тренировки: добавьте lr sheduling, early stopping, поэкспериментируйте с оптимизатором - **2 балла**\n",
    "- поэкспериментируйте с сэмплированием - замените greedy-инференс на альтернативные варианты - **2 балла**\n",
    "- проведите ablation-study, сделайте выводы о влиянии ваших изменений на итоговую производительность модели - **2 балла**\n",
    "\n",
    "**Общее**\n",
    "\n",
    "- Принимаемые решения обоснованы (почему выбрана определенная архитектура/гиперпараметр/оптимизатор/преобразование и т.п.) - **1 балл**\n",
    "- Обеспечена воспроизводимость решения: зафиксированы random_state, ноутбук воспроизводится от начала до конца без ошибок - **1 балл**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 0 - Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импортируем необходимые библиотеки и компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Generator\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from razdel import tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import trange\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фиксируем seed'ы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1 - Размер словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составим файлы с текстами разных языков как на занятии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cordelia Hotel is situated in Tbilisi, a 3-minute walk away from Saint Trinity Church.\tОтель Cordelia расположен в Тбилиси, в 3 минутах ходьбы от Свято-Троицкого собора.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/raw/data.txt\", encoding=\"utf-8\") as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cordelia hotel is situated in tbilisi , a 3-minute walk away from saint trinity church .\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(s: str) -> str:\n",
    "    return \" \".join(list(t.text for t in tokenize(s.lower())))\n",
    "\n",
    "\n",
    "def read_file_lines(file_path: str, encoding=\"utf-8\") -> Generator[str, None, None]:\n",
    "    with open(file_path, encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "\n",
    "\n",
    "print(tokenize_text(\"Cordelia Hotel is situated in Tbilisi, a 3-minute walk away from Saint Trinity Church.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_base_path = Path(\"../data/hw_2/\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(artifact_base_path / \"train.en\", \"w\", encoding=\"utf-8\") as f_src, open(\n",
    "    artifact_base_path / \"train.ru\", \"w\", encoding=\"utf-8\"\n",
    ") as f_dst:\n",
    "\n",
    "    for line in read_file_lines(\"../data/raw/data.txt\"):\n",
    "        src_line, dst_line = line.strip().split(\"\\t\")\n",
    "        print(tokenize_text(src_line), file=f_src)\n",
    "        print(tokenize_text(dst_line), file=f_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bpe(num_symbols: int, lang: str) -> Path:\n",
    "    input_file = artifact_base_path / f\"train.{lang}\"\n",
    "    rules_file = artifact_base_path / f\"bpe_rules_{num_symbols}.{lang}\"\n",
    "\n",
    "    learn_bpe(open(input_file, encoding=\"utf-8\"), open(rules_file, \"w\", encoding=\"utf-8\"), num_symbols=num_symbols)\n",
    "    bpe = BPE(open(rules_file, encoding=\"utf-8\"))\n",
    "\n",
    "    processed_text = artifact_base_path / f\"train.bpe_{num_symbols}.{lang}\"\n",
    "\n",
    "    with open(processed_text, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for line in read_file_lines(input_file):\n",
    "            print(bpe.process_line(line.strip()), file=f_out)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens, bos=\"_BOS_\", eos=\"_EOS_\", unk=\"_UNK_\"):\n",
    "        \"\"\"\n",
    "        A special class that converts lines of tokens into matrices and backwards\n",
    "        \"\"\"\n",
    "        assert all(tok in tokens for tok in (bos, eos, unk))\n",
    "        self.tokens = tokens\n",
    "        self.token_to_ix = {t: i for i, t in enumerate(tokens)}\n",
    "        self.bos, self.eos, self.unk = bos, eos, unk\n",
    "        self.bos_ix = self.token_to_ix[bos]\n",
    "        self.eos_ix = self.token_to_ix[eos]\n",
    "        self.unk_ix = self.token_to_ix[unk]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_lines(lines, bos=\"_BOS_\", eos=\"_EOS_\", unk=\"_UNK_\"):\n",
    "        flat_lines = \"\\n\".join(list(lines)).split()\n",
    "        tokens = sorted(set(flat_lines))\n",
    "        tokens = [t for t in tokens if t not in (bos, eos, unk) and len(t)]\n",
    "        tokens = [bos, eos, unk] + tokens\n",
    "        return Vocab(tokens, bos, eos, unk)\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        \"\"\"converts string to a list of tokens\"\"\"\n",
    "        tokens = [tok if tok in self.token_to_ix else self.unk for tok in string.split()]\n",
    "        return [self.bos] + tokens + [self.eos]\n",
    "\n",
    "    def to_matrix(self, lines, dtype=torch.int64, max_len=None):\n",
    "        \"\"\"\n",
    "        convert variable length token sequences into  fixed size matrix\n",
    "        example usage:\n",
    "        >>>print(to_matrix(words[:3],source_to_ix))\n",
    "        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n",
    "         [30 21 15 15 21 14 28 27 13 -1 -1]\n",
    "         [25 37 31 34 21 20 37 21 28 19 13]]\n",
    "        \"\"\"\n",
    "        lines = list(map(self.tokenize, lines))\n",
    "        max_len = max_len or max(map(len, lines))\n",
    "\n",
    "        matrix = torch.full((len(lines), max_len), self.eos_ix, dtype=dtype)\n",
    "        for i, seq in enumerate(lines):\n",
    "            row_ix = list(map(self.token_to_ix.get, seq))[:max_len]\n",
    "            matrix[i, : len(row_ix)] = torch.as_tensor(row_ix)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def to_lines(self, matrix, crop=True):\n",
    "        \"\"\"\n",
    "        Convert matrix of token ids into strings\n",
    "        :param matrix: matrix of tokens of int32, shape=[batch,time]\n",
    "        :param crop: if True, crops BOS and EOS from line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for line_ix in map(list, matrix):\n",
    "            if crop:\n",
    "                if line_ix[0] == self.bos_ix:\n",
    "                    line_ix = line_ix[1:]\n",
    "                if self.eos_ix in line_ix:\n",
    "                    line_ix = line_ix[: line_ix.index(self.eos_ix)]\n",
    "            line = \" \".join(self.tokens[i] for i in line_ix)\n",
    "            lines.append(line)\n",
    "        return lines\n",
    "\n",
    "    def compute_mask(self, input_ix):\n",
    "        \"\"\"compute a boolean mask that equals \"1\" until first EOS (including that EOS)\"\"\"\n",
    "        return F.pad(torch.cumsum(input_ix == self.eos_ix, dim=-1)[..., :-1] < 1, pad=(1, 0, 0, 0), value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим разные размеры словаря\n",
    "\n",
    "Для скорости проверки возьмём 3 - 8k как на паре, 8k * 2 и 8k * 4. \n",
    "\n",
    "Попробуем найти оптимальный по соотношению длина последовательности / размер словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = [8192, 16384, 32768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8192/8192 [00:04<00:00, 1771.83it/s]\n",
      "100%|██████████| 8192/8192 [00:05<00:00, 1586.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vocab Size 8192 ===\n",
      "Input vocab size: 8009\n",
      "Output vocab size: 8009\n",
      "Average token count - Input: 17.71721276595745 | Output: 17.71721276595745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16384/16384 [00:11<00:00, 1376.58it/s]\n",
      "100%|██████████| 16384/16384 [00:12<00:00, 1334.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vocab Size 16384 ===\n",
      "Input vocab size: 14799\n",
      "Output vocab size: 14799\n",
      "Average token count - Input: 17.01168085106383 | Output: 17.01168085106383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 23588/32768 [00:16<00:02, 4184.45it/s]no pair has frequency >= 2. Stopping\n",
      " 73%|███████▎  | 24060/32768 [00:17<00:06, 1403.09it/s]\n",
      "100%|██████████| 32768/32768 [00:28<00:00, 1169.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vocab Size 32768 ===\n",
      "Input vocab size: 20442\n",
      "Output vocab size: 20442\n",
      "Average token count - Input: 16.695531914893618 | Output: 16.695531914893618\n"
     ]
    }
   ],
   "source": [
    "for selected_vocab_size in vocab_sizes:\n",
    "    train_bpe_en = apply_bpe(selected_vocab_size, \"en\")\n",
    "    train_bpe_ru = apply_bpe(selected_vocab_size, \"ru\")\n",
    "\n",
    "    data_inp = np.array(open(train_bpe_en, encoding=\"utf-8\").read().split(\"\\n\"))\n",
    "    data_out = np.array(open(train_bpe_en, encoding=\"utf-8\").read().split(\"\\n\"))\n",
    "\n",
    "    data_inp = data_inp[data_inp != \"\"]\n",
    "    data_out = data_out[data_out != \"\"]\n",
    "\n",
    "    train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000, random_state=seed)\n",
    "\n",
    "    inp_voc = Vocab.from_lines(train_inp)\n",
    "    out_voc = Vocab.from_lines(train_out)\n",
    "\n",
    "    print(f\"\\n=== Vocab Size {selected_vocab_size} ===\")\n",
    "    print(\"Input vocab size:\", len(inp_voc))\n",
    "    print(\"Output vocab size:\", len(out_voc))\n",
    "\n",
    "    avg_tokens_inp = np.mean([len(line.split()) for line in train_inp])\n",
    "    avg_tokens_out = np.mean([len(line.split()) for line in train_out])\n",
    "    print(\"Average token count - Input:\", avg_tokens_inp, \"| Output:\", avg_tokens_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "\n",
    "Увеличение размера словаря снижает среднюю длину последовательности (с 17.72 токенов при 8192 до 16.70 при 32768), что положительно сказывается на вычислительной сложности. \n",
    "\n",
    "При этом прирост снижения длины от 16k до 32k минимален, а размер словаря значительно растёт. Оптимальным выглядит около 16k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16384/16384 [00:11<00:00, 1434.14it/s]\n",
      "100%|██████████| 16384/16384 [00:11<00:00, 1405.21it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_vocab_size = 16384\n",
    "\n",
    "train_bpe_en = apply_bpe(selected_vocab_size, \"en\")\n",
    "train_bpe_ru = apply_bpe(selected_vocab_size, \"ru\")\n",
    "\n",
    "data_inp = np.array(open(artifact_base_path / train_bpe_ru, encoding=\"utf-8\").read().split(\"\\n\"))\n",
    "data_out = np.array(open(artifact_base_path / train_bpe_en, encoding=\"utf-8\").read().split(\"\\n\"))\n",
    "\n",
    "data_inp = data_inp[data_inp != \"\"]\n",
    "data_out = data_out[data_out != \"\"]\n",
    "\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000, random_state=2023)\n",
    "\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2 - Эксперименты с моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовые модели с лекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        A simple encoder-decoder seq2seq model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "\n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "\n",
    "    def forward(self, inp, out):\n",
    "        \"\"\"Apply model in training mode\"\"\"\n",
    "        initial_state = self.encode(inp)\n",
    "        return self.decode(initial_state, out)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :returns: initial decoder state tensors, one or many\n",
    "        \"\"\"\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "\n",
    "        enc_seq, last_state_but_not_really = self.enc0(inp_emb)\n",
    "        # enc_seq: [batch, sequence length, hid_size], last_state: [batch, hid_size]\n",
    "\n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "\n",
    "        dec_start = self.dec_start(last_state)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]\n",
    "        \"\"\"\n",
    "        prev_gru0_state = prev_state[0]\n",
    "        prev_emb = self.emb_out(prev_tokens)\n",
    "        new_dec_state = self.dec0(prev_emb, prev_gru0_state)  # input & hidden states\n",
    "        output_logits = self.logits(new_dec_state)\n",
    "\n",
    "        return [new_dec_state], output_logits\n",
    "\n",
    "    def decode(self, initial_state, out_tokens, **flags):\n",
    "        \"\"\"Iterate over reference tokens (out_tokens) with decode_step\"\"\"\n",
    "        batch_size = out_tokens.shape[0]\n",
    "        state = initial_state\n",
    "\n",
    "        # initial logits: always predict BOS\n",
    "        onehot_bos = F.one_hot(\n",
    "            torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64), num_classes=len(self.out_voc)\n",
    "        ).to(device=out_tokens.device)\n",
    "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
    "\n",
    "        logits_sequence = [first_logits]\n",
    "        for i in range(out_tokens.shape[1] - 1):\n",
    "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
    "            logits_sequence.append(logits)\n",
    "        return torch.stack(logits_sequence, dim=1)\n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\"Generate translations from model (greedy version)\"\"\"\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = initial_state\n",
    "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64, device=device)]\n",
    "        all_states = [initial_state]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            state, logits = self.decode_step(state, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            all_states.append(state)\n",
    "\n",
    "        return torch.stack(outputs, dim=1), all_states\n",
    "\n",
    "    def translate_lines(self, inp_lines, **kwargs):\n",
    "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "        initial_state = self.encode(inp)\n",
    "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, enc_size, dec_size, hid_size):\n",
    "        \"\"\"A layer that computes additive attention response and weights\"\"\"\n",
    "        super().__init__()\n",
    "        self.enc_size = enc_size  # num units in encoder state\n",
    "        self.dec_size = dec_size  # num units in decoder state\n",
    "        self.hid_size = hid_size  # attention layer hidden units\n",
    "\n",
    "        self.linear_enc = nn.Linear(enc_size, hid_size)\n",
    "        self.linear_dec = nn.Linear(dec_size, hid_size)\n",
    "        self.linear_out = nn.Linear(hid_size, 1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "        batch_size, ninp, enc_size = enc.shape\n",
    "\n",
    "        # Compute logits\n",
    "        x = self.linear_dec(dec).reshape(-1, 1, self.hid_size)\n",
    "        x = torch.tanh(self.linear_enc(enc) + x)\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "        # You may need torch.where\n",
    "        x[torch.where(inp_mask == False)] = -1e9\n",
    "\n",
    "        # Compute attention probabilities (softmax)\n",
    "        probs = self.softmax(x.reshape(batch_size, ninp))\n",
    "\n",
    "        # Compute attention response using enc and probs\n",
    "        attn = (probs.reshape(batch_size, ninp, 1) * enc).sum(1)\n",
    "\n",
    "        return attn, probs\n",
    "\n",
    "\n",
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\"Translation model that uses attention. See instructions above.\"\"\"\n",
    "        super().__init__(inp_voc, out_voc, emb_size, hid_size)\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "\n",
    "        self.enc0 = nn.LSTM(emb_size, hid_size, num_layers=2, batch_first=True)\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "\n",
    "        self.dec0 = nn.GRUCell(emb_size + hid_size, hid_size)\n",
    "        self.attention = AttentionLayer(hid_size, hid_size, attn_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes input sequences, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        # encode input sequence, create initial decoder states\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        enc_seq, last_state_but_not_really = self.enc0(inp_emb)\n",
    "        # [dec_start] = super().encode(inp, **flags)\n",
    "\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        dec_start = self.dec_start(last_state)\n",
    "\n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        inp_mask = self.out_voc.compute_mask(inp)\n",
    "        first_attn_probas = self.attention(enc_seq, dec_start, inp_mask)\n",
    "\n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "\n",
    "        first_state = [dec_start, enc_seq, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "\n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        attn, attn_probs = self.attention(enc_seq, prev_gru0_state, enc_mask)\n",
    "\n",
    "        x = self.emb_out(prev_tokens)\n",
    "        x = torch.cat([attn, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "\n",
    "        new_dec_state = [x, enc_seq, enc_mask, attn_probs]\n",
    "        output_logits = self.logits(x)\n",
    "        return [new_dec_state, output_logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составим свои модели по заданию\n",
    "\n",
    "1. Используем GRU вместо LSTM\n",
    "2. Используем bi-directional LSTM\n",
    "\n",
    "Не будем создавать слишком много моделей для оптимизации процесса обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveModel_GRU(BasicModel):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\"Translation model with a 2-layer GRU encoder and GRUCell decoder.\"\"\"\n",
    "        super().__init__(inp_voc, out_voc, emb_size, hid_size)\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, num_layers=2, batch_first=True)\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size + hid_size, hid_size)\n",
    "        self.attention = AttentionLayer(hid_size, hid_size, attn_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        enc_seq, last_state = self.enc0(inp_emb)\n",
    "        last_state = last_state[-1]\n",
    "        dec_start = self.dec_start(last_state)\n",
    "        inp_mask = self.out_voc.compute_mask(inp)\n",
    "        first_attn = self.attention(enc_seq, dec_start, inp_mask)\n",
    "        return [dec_start, enc_seq, inp_mask, first_attn]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        attn, attn_probs = self.attention(enc_seq, prev_gru0_state, enc_mask)\n",
    "        x = self.emb_out(prev_tokens)\n",
    "        x = torch.cat([attn, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "        new_state = [x, enc_seq, enc_mask, attn_probs]\n",
    "        output_logits = self.logits(x)\n",
    "        return [new_state, output_logits]\n",
    "\n",
    "\n",
    "class AttentiveModel_Bidirectional(BasicModel):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\"Translation model with a 2-layer bidirectional LSTM encoder and GRUCell decoder.\"\"\"\n",
    "        super().__init__(inp_voc, out_voc, emb_size, hid_size)\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "\n",
    "        self.enc0 = nn.LSTM(emb_size, hid_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)  # combining forward and backward states\n",
    "        self.attn_proj = nn.Linear(hid_size * 2, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size + hid_size, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        self.attention = AttentionLayer(hid_size * 2, hid_size, attn_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        enc_seq, (h_n, c_n) = self.enc0(inp_emb)\n",
    "        forward = h_n[-2, :, :]\n",
    "        backward = h_n[-1, :, :]\n",
    "        last_state = forward + backward\n",
    "        dec_start = self.dec_start(last_state)\n",
    "        inp_mask = self.out_voc.compute_mask(inp)\n",
    "        first_attn = self.attention(enc_seq, dec_start, inp_mask)\n",
    "        return [dec_start, enc_seq, inp_mask, first_attn]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        attn, attn_probs = self.attention(enc_seq, prev_gru0_state, enc_mask)\n",
    "        attn_proj = self.attn_proj(attn)\n",
    "        x = self.emb_out(prev_tokens)\n",
    "        x = torch.cat([attn_proj, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "        new_state = [x, enc_seq, enc_mask, attn_probs]\n",
    "        output_logits = self.logits(x)\n",
    "        return [new_state, output_logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    Compute loss (float32 scalar) as in the formula above\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "\n",
    "    In order to pass the tests, your function should\n",
    "    * include loss at first EOS but not the subsequent ones\n",
    "    * divide sum of losses by a sum of input lengths (use voc.compute_mask)\n",
    "    \"\"\"\n",
    "    mask = model.out_voc.compute_mask(out)  # [batch_size, out_len]\n",
    "    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
    "\n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    logits_seq = model(inp, out)\n",
    "\n",
    "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
    "    logprobs_seq = torch.log_softmax(logits_seq, dim=-1)\n",
    "\n",
    "    # log-probabilities of correct outputs, [batch_size, out_len]\n",
    "    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1)\n",
    "    # ^-- this will select the probability of the actual next token.\n",
    "    # Note: you can compute loss more efficiently using using F.cross_entropy\n",
    "\n",
    "    # average cross-entropy over tokens where mask == True\n",
    "    return -logp_out[mask].mean()\n",
    "\n",
    "\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep=\"@@ \", **flags):\n",
    "    \"\"\"\n",
    "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "        translations = [line.replace(bpe_sep, \"\") for line in translations]\n",
    "        actual = [line.replace(bpe_sep, \"\") for line in out_lines]\n",
    "        return (\n",
    "            corpus_bleu(\n",
    "                [[ref.split()] for ref in actual],\n",
    "                [trans.split() for trans in translations],\n",
    "                smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions],\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_inp,\n",
    "    train_out,\n",
    "    dev_inp,\n",
    "    dev_out,\n",
    "    inp_voc,\n",
    "    out_voc,\n",
    "    n_steps=5000,\n",
    "    batch_size=32,\n",
    "    eval_interval=100,\n",
    "):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    metrics = {\"train_loss\": [], \"dev_bleu\": []}\n",
    "    for step in trange(n_steps):\n",
    "        batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "        batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "        batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = compute_loss(model, batch_inp, batch_out)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        metrics[\"train_loss\"].append(loss.item())\n",
    "        if step % eval_interval == 0:\n",
    "            bleu = compute_bleu(model, dev_inp, dev_out)\n",
    "            metrics[\"dev_bleu\"].append(bleu)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:00<00:00, 13.87it/s]\n"
     ]
    }
   ],
   "source": [
    "model_lstm = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "metrics_lstm = train_model(model_lstm, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.255\n"
     ]
    }
   ],
   "source": [
    "final_bleu_lstm = np.mean(metrics_lstm[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_lstm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:54<00:00, 14.11it/s]\n"
     ]
    }
   ],
   "source": [
    "model_gru = AttentiveModel_GRU(inp_voc, out_voc).to(device)\n",
    "metrics_gru = train_model(model_gru, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.015\n"
     ]
    }
   ],
   "source": [
    "final_bleu_gru = np.mean(metrics_gru[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_gru:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:29<00:00, 12.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_bi = AttentiveModel_Bidirectional(inp_voc, out_voc).to(device)\n",
    "metrics_bi = train_model(model_bi, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.159\n"
     ]
    }
   ],
   "source": [
    "final_bleu_bi = np.mean(metrics_bi[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_bi:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "\n",
    "Двунаправленный LSTM (BLEU ~ 14.15) значительно улучшает качество перевода по сравнению с базовой моделью (BLEU ~ 11.25) и моделью с GRU-энкодером (BLEU ~ 12.02). Это подтверждает, что учет контекста с обеих сторон (bidirectional) положительно влияет на результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 4 - Оптимизация процесса обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим указанные в задании методы в процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_improved(\n",
    "    model,\n",
    "    train_inp,\n",
    "    train_out,\n",
    "    dev_inp,\n",
    "    dev_out,\n",
    "    inp_voc,\n",
    "    out_voc,\n",
    "    n_steps=5000,\n",
    "    batch_size=32,\n",
    "    eval_interval=100,\n",
    "    patience=5,\n",
    "    optimizer_choice=\"adam\",\n",
    "):\n",
    "\n",
    "    if optimizer_choice == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=patience, verbose=True\n",
    "    )\n",
    "\n",
    "    metrics = {\"train_loss\": [], \"dev_bleu\": []}\n",
    "    best_bleu = -float(\"inf\")\n",
    "    best_step = 0\n",
    "\n",
    "    for step in trange(n_steps):\n",
    "        batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "        batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "        batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, batch_inp, batch_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics[\"train_loss\"].append(loss.item())\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            bleu = compute_bleu(model, dev_inp, dev_out)\n",
    "            metrics[\"dev_bleu\"].append(bleu)\n",
    "            scheduler.step(bleu)\n",
    "\n",
    "            if bleu > best_bleu:\n",
    "                best_bleu = bleu\n",
    "                best_step = step\n",
    "            elif step - best_step > patience * eval_interval:\n",
    "                print(f\"Early stopping at step {step}, best BLEU: {best_bleu:.2f}\")\n",
    "                break\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:01<00:00, 13.83it/s]\n"
     ]
    }
   ],
   "source": [
    "model_lstm_adam = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "metrics_lstm_adam = train_model_improved(\n",
    "    model_lstm_adam, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.442\n"
     ]
    }
   ],
   "source": [
    "final_bleu_lstm_adam = np.mean(metrics_lstm_adam[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_lstm_adam:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2500/5000 [03:04<03:04, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 2500, best BLEU: 2.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm_sgd = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "metrics_lstm_sgd = train_model_improved(\n",
    "    model_lstm_sgd, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000, optimizer_choice=\"sgd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.159\n"
     ]
    }
   ],
   "source": [
    "final_bleu_lstm_sgd = np.mean(metrics_lstm_sgd[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_lstm_sgd:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:01<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_gru_adam = AttentiveModel_GRU(inp_voc, out_voc).to(device)\n",
    "metrics_gru_adam = train_model_improved(\n",
    "    model_gru_adam, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.390\n"
     ]
    }
   ],
   "source": [
    "final_bleu_gru_adam = np.mean(metrics_gru_adam[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_gru_adam:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2100/5000 [02:32<03:30, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 2100, best BLEU: 15.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_gru_sgd = AttentiveModel_GRU(inp_voc, out_voc).to(device)\n",
    "metrics_gru_sgd = train_model_improved(\n",
    "    model_gru_adam, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000, optimizer_choice=\"sgd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.727\n"
     ]
    }
   ],
   "source": [
    "final_bleu_gru_sgd = np.mean(metrics_gru_sgd[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_gru_sgd:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:43<00:00, 12.40it/s]\n"
     ]
    }
   ],
   "source": [
    "model_bi_adam = AttentiveModel_Bidirectional(inp_voc, out_voc).to(device)\n",
    "metrics_bi_adam = train_model_improved(\n",
    "    model_bi_adam, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.144\n"
     ]
    }
   ],
   "source": [
    "final_bleu_bi_adam = np.mean(metrics_bi_adam[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_bi_adam:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4000/5000 [05:19<01:19, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 4000, best BLEU: 3.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_bi_sgd = AttentiveModel_Bidirectional(inp_voc, out_voc).to(device)\n",
    "metrics_bi_sgd = train_model_improved(\n",
    "    model_bi_sgd, train_inp, train_out, dev_inp, dev_out, inp_voc, out_voc, n_steps=5000, optimizer_choice=\"sgd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.743\n"
     ]
    }
   ],
   "source": [
    "final_bleu_bi_sgd = np.mean(metrics_bi_sgd[\"dev_bleu\"][-10:])\n",
    "print(f\"{final_bleu_bi_sgd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "\n",
    "Получили вполне интересные результаты: оптимизация обучения с early stopping, ~16 BLEU для GRU модели с SGD оптимизатором.\n",
    "\n",
    "Но были результаты и значительно хуже (~3 BLEU). Возможно, мы попали в плохой локальный минимум или выбрали неудачные параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 5 - Эксперименты с inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем не только `greedy`, но и `sampling` методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, inp_lines, max_len=100, method=\"greedy\", top_k=10, temperature=1.0):\n",
    "    inp = model.inp_voc.to_matrix(inp_lines).to(device)\n",
    "    state = model.encode(inp)\n",
    "    batch_size = state[0].shape[0]\n",
    "\n",
    "    outputs = [torch.full([batch_size], model.out_voc.bos_ix, dtype=torch.int64, device=device)]\n",
    "    all_states = [state]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        state, logits = model.decode_step(state, outputs[-1])\n",
    "        if method == \"greedy\":\n",
    "            next_tokens = logits.argmax(dim=-1)\n",
    "        elif method == \"sampling\":\n",
    "            scaled_logits = logits / temperature\n",
    "            top_logits, top_indices = torch.topk(scaled_logits, top_k, dim=-1)\n",
    "            probs = F.softmax(top_logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, 1).squeeze(dim=-1)\n",
    "            next_tokens = top_indices.gather(dim=-1, index=next_idx.unsqueeze(-1)).squeeze(-1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown decoding method\")\n",
    "        outputs.append(next_tokens)\n",
    "        all_states.append(state)\n",
    "\n",
    "    output_ids = torch.stack(outputs, dim=1)\n",
    "    translations = model.out_voc.to_lines(output_ids.cpu().numpy())\n",
    "    return translations, all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['всем гостям предоставляются японские халаты юката . на территории предусмотрена бесплатная парковка .',\n",
       "       'апартаменты casa della g@@ hi@@ anda@@ ia расположены в 5 минутах ходьбы от ближайшего песчаного пляжа в порто-@@ чезаре@@ о . к услугам гостей апартаменты с кондиционером и телевизором с плоским экраном .',\n",
       "       'в каждом номере отеля cand@@ i@@ ani имеется кондиционер , мини-бар и телевизор со спутниковыми каналами .',\n",
       "       'поездка до ресторана lang@@ dal@@ es viney@@ ard занимает 20 минут .',\n",
       "       'в лобби-баре некоторые местные алкогольные и безалкогольные напитки подаются бесплатно .'],\n",
       "      dtype='<U490')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_inputs = dev_inp[:5]\n",
    "sample_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding results:\n",
      "free parking is available on site .\n",
      "located in a quiet area , just a 5-minute walk from the beach , this apartment features a balcony with a terrace .\n",
      "all rooms at the hotel feature a flat-screen tv , air conditioning and a minibar .\n",
      "the hotel is a 10-minute drive from the hotel .\n",
      "the bar offers a variety of drinks and snacks .\n"
     ]
    }
   ],
   "source": [
    "greedy_translations, _ = perform_inference(model_lstm_adam, sample_inputs, max_len=25, method=\"greedy\")\n",
    "print(\"Greedy decoding results:\")\n",
    "for t in greedy_translations:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling decoding results:\n",
      "free wifi is available throughout the hotel .\n",
      "featuring a garden terrace , villa casa is a self-catering accommodation located in a quiet area .\n",
      "all rooms at the hotel ' s apartments have a flat-screen tv and a minibar .\n",
      "it takes just off the town of rome , is 12 miles away .\n",
      "the lobby bar serves breakfast and copy , .\n"
     ]
    }
   ],
   "source": [
    "sampling_translations, _ = perform_inference(\n",
    "    model_lstm_adam, sample_inputs, max_len=25, method=\"sampling\", top_k=10, temperature=1.0\n",
    ")\n",
    "print(\"Sampling decoding results:\")\n",
    "for t in sampling_translations:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "\n",
    "Очевидно, качество перевода оставляет желать лучшего. \n",
    "\n",
    "Но также видно, что метод `sampling` значительно разнообразил тексты, хотя и качество умозрительно кажется ниже, чем с `greedy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 6 - Финальные выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Размер словаря:**\n",
    "Увеличение размера словаря с использованием BPE снижает среднюю длину последовательности – с ~17.72 токенов при 8192 до ~16.70 при 32768. Однако переход от 16k к 32k дает минимальное сокращение длины при существенном увеличении словаря, поэтому оптимальным кажется ~16k.\n",
    "\n",
    "2. **Архитектура модели:**\n",
    "Применение двунаправленного LSTM заметно улучшает качество перевода (BLEU ~ 14.15) по сравнению с базовой моделью (BLEU ~ 11.25) и моделью с GRU-энкодером (BLEU ~ 12.02). Это подтверждает, что учет контекста с обеих сторон положительно влияет на результат.\n",
    "\n",
    "3. **Оптимизация обучения:**\n",
    "Улучшения с early stopping и использованием, например, SGD для GRU-модели дали интересный результат (~16 BLEU). Но встречались случаи, когда качество было крайне низким (~3 BLEU), что может говорить о попадании в плохой локальный минимум или неподходящие гиперпараметрах.\n",
    "\n",
    "4. **Методы инференса:**\n",
    "Метод `sampling` обеспечивает разнообразие переводов, однако по умозрительной оценке их качество уступает `greedy` версии. Возможно, были выбраны не лучшие параметры для `sampling`.\n",
    "\n",
    "В итоге, мы видим, что результаты работы на занятии можно улучшить с помощью описанных шагов, что и было сделано."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
